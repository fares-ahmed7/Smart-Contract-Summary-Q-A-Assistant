# ğŸš€ Smart Contract Summary & Q&A Assistant (RAG + Evaluation)

## ğŸ“Œ Overview
**Smart Contract Summary & Q&A Assistant** is a local Retrieval-Augmented Generation (RAG) system with a built-in evaluation framework. 
It allows users to upload long documents (PDF/DOCX), ask precise questions, and receive answers strictly grounded in the document content, complete with **page-level citations** and **automatic quality scoring**.

The system is designed with **strong guardrails** to prevent hallucinations and includes an **LLM-based evaluator** to score answer accuracy, faithfulness, and compliance.

---

## ğŸ¯ Key Capabilities
* âœ… **Multi-format Ingestion:** Upload and index PDF & DOCX documents.
* âœ… **Page-aware Parsing:** Automatic [[PAGE X]] tagging for precise citations.
* âœ… **Semantic Chunking:** Advanced text splitting with overlap for context retention.
* âœ… **Vector Search:** High-performance similarity search using ChromaDB.
* âœ… **Anti-Hallucination:** Strict system prompts ensuring context exclusivity.
* âœ… **Built-in Evaluation:** Automatic scoring for Accuracy, Faithfulness, and Citations.
* âœ… **Local Microservice:** Powered by FastAPI and LangServe.
* âœ… **Interactive UI:** Clean Gradio interface for seamless chat experience.

---

## ğŸ§  System Pipeline

### ğŸ”¹ Question Answering Flow
1. **Ingestion:** Documents are parsed and split into semantic chunks.
2. **Embedding:** Chunks are converted to vectors using `sentence-transformers`.
3. **Retrieval:** Relevant context is fetched from ChromaDB based on user query.
4. **Generation:** LLaMA 3.1 8B generates an answer based *only* on retrieved context.

### ğŸ”¹ Evaluation Metrics (LLM-as-a-Judge)
After answering, the system evaluates the response using:
* **Answer Accuracy:** Does it answer the user's intent?
* **Faithfulness:** Is the answer derived solely from the provided context?
* **Citation Accuracy:** Are the page numbers correct?
* **Guard-Rail Compliance:** Did the model follow safety/exclusion rules?

---

## ğŸ“‚ Project Structure
```
Smart-Contract-Summary-Q-A-Assistant/
â”œâ”€â”€ server_app.py      # FastAPI backend + RAG & Evaluation chains
â”œâ”€â”€ app.py             # Gradio UI (Frontend)
â”œâ”€â”€ .gitignore         # Excludes local storage, env files, and cache
â”œâ”€â”€ requirements.txt   # Project dependencies
â””â”€â”€ README.md          # Documentation
```

---

## ğŸ›  Technology Stack

| Category       | Tool                                             |
|----------------|-------------------------------------------------|
| Backend API    | FastAPI + LangServe                              |
| RAG Framework  | LangChain                                        |
| Frontend       | Gradio                                           |
| Vector Store   | ChromaDB                                         |
| Embeddings     | sentence-transformers/all-MiniLM-L6-v2          |
| LLM Model      | LLaMA 3.1 8B Instruct (via HF Inference API)   |

---

## 2ï¸âƒ£ Environment Setup

```bash
python -m venv .venv
# Activate:
# Windows: .venv\Scripts\activate
# Linux/macOS: source .venv/bin/activate
pip install -r requirements.txt
```

---

## 3ï¸âƒ£ Configuration
Create a `.env` file in the root directory:
```env
HF_TOKEN=your_huggingface_token_here
```

---

## â–¶ï¸ Running the Application

### Step 1: Start Backend Server
```bash
uvicorn server_app:app --host 127.0.0.1 --port 9017
```

### Step 2: Launch Frontend (Gradio)
In a separate terminal, run:
```bash
python app.py
```
The Gradio interface will open in your browser automatically.

---

## ğŸ§‘â€ğŸ’» Author
**Fares Ahmed**  
AI/ML Engineer
